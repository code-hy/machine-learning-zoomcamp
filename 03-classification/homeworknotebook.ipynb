import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import mutual_info_score

# Load the data
url = "https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv"
df = pd.read_csv(url)

# Handle missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna('NA')
    else:
        df[col] = df[col].fillna(0.0)

# Question 1: What is the most frequent observation (mode) for the column industry?
industry_mode = df['industry'].mode()[0]
print(f"Question 1: The mode of the 'industry' column is: {industry_mode}")

# Question 2: Create the correlation matrix for the numerical features
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns
correlation_matrix = df[numerical_features].corr()

# Check specific pairs
pairs = [
    ('interaction_count', 'lead_score'),
    ('number_of_courses_viewed', 'lead_score'),
    ('number_of_courses_viewed', 'interaction_count'),
    ('annual_income', 'interaction_count')
]

max_corr = 0
max_pair = None
for pair in pairs:
    corr = correlation_matrix.loc[pair[0], pair[1]]
    if abs(corr) > max_corr:
        max_corr = abs(corr)
        max_pair = pair

print(f"Question 2: The pair with the highest correlation is: {max_pair} with correlation {max_corr}")

# Split the data
X = df.drop('converted', axis=1)
y = df['converted']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Question 3: Calculate the mutual information score between y and other categorical variables
categorical_features = X_train.select_dtypes(include=['object']).columns
mi_scores = {}
for feature in categorical_features:
    mi_score = mutual_info_score(y_train, X_train[feature])
    mi_scores[feature] = round(mi_score, 2)

max_mi_feature = max(mi_scores, key=mi_scores.get)
print(f"Question 3: The variable with the biggest mutual information score is: {max_mi_feature}")

# Question 4: Train a logistic regression model and calculate accuracy
# One-hot encoding
dv = DictVectorizer(sparse=False)
train_dict = X_train.to_dict(orient='records')
X_train_encoded = dv.fit_transform(train_dict)

val_dict = X_val.to_dict(orient='records')
X_val_encoded = dv.transform(val_dict)

# Train the model
model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)
model.fit(X_train_encoded, y_train)

# Calculate accuracy
y_pred = model.predict(X_val_encoded)
accuracy = accuracy_score(y_val, y_pred)
print(f"Question 4: Accuracy: {round(accuracy, 2)}")

# Question 5: Find the least useful feature using feature elimination
# Get original accuracy
original_accuracy = accuracy_score(y_val, model.predict(X_val_encoded))

# Get feature names
feature_names = dv.get_feature_names_out()

# Test each feature
feature_importance = {}
for i, feature in enumerate(feature_names):
    # Create a new dataset without this feature
    X_train_without = np.delete(X_train_encoded, i, axis=1)
    X_val_without = np.delete(X_val_encoded, i, axis=1)
    
    # Train a new model
    model_without = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)
    model_without.fit(X_train_without, y_train)
    
    # Calculate accuracy
    y_pred_without = model_without.predict(X_val_without)
    accuracy_without = accuracy_score(y_val, y_pred_without)
    
    # Calculate difference
    diff = original_accuracy - accuracy_without
    feature_importance[feature] = diff

# Find the feature with the smallest difference among the given options
options = ['industry', 'employment_status', 'lead_score']
min_diff = float('inf')
least_useful = None
for option in options:
    # Find all features that start with the option name
    matching_features = [f for f in feature_names if f.startswith(option)]
    # Calculate the average difference for this option
    avg_diff = np.mean([feature_importance[f] for f in matching_features])
    if abs(avg_diff) < min_diff:
        min_diff = abs(avg_diff)
        least_useful = option

print(f"Question 5: The least useful feature is: {least_useful}")

# Question 6: Train regularized logistic regression with different C values
# Test different C values
C_values = [0.01, 0.1, 1, 10, 100]
accuracies = {}

for C in C_values:
    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)
    model.fit(X_train_encoded, y_train)
    y_pred = model.predict(X_val_encoded)
    accuracy = accuracy_score(y_val, y_pred)
    accuracies[C] = round(accuracy, 3)

# Find the C with the best accuracy
best_C = max(accuracies, key=accuracies.get)
print(f"Question 6: The best C value is: {best_C}")
